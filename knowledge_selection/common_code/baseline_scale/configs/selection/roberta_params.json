{
    "dataset_args": {
      "n_candidates": 4,
      "eval_all_snippets": false,
      "negative_sample_method": "oracle",
      
      "history_max_utterances": 1000000,
      "history_max_tokens": 128,
      "knowledge_max_tokens": 128,
      "selection_type" : "all",
      "candidate_num":64
    },
    
  
    "task": "selection",
    "model_name_or_path": "roberta-base",
  
    "per_gpu_train_batch_size": 8,
    "per_gpu_eval_batch_size": 64,
    "per_gpu_sample_batch_size":1,
    "gradient_accumulation_steps": 4,
    "max_candidates_per_forward_eval": 512,
    
    "learning_rate": 1e-5,
    "adam_epsilon": 1e-8,
    "max_grad_norm": 10.0,
  
    "num_train_epochs": 50,
    "warmup_steps": 0,
  
    "fp16": "",
  
    "seed": 42
  }
  